{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a6239de-342a-4a87-a4af-bde680caa7a2",
   "metadata": {},
   "source": [
    "# **Easy Speech-to-Text with Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5607bce-97c6-4c14-bb16-a03fdb3a1a9a",
   "metadata": {},
   "source": [
    "Let’s say you are a podcast creator, and you want to transcribe your podcast, so that it can be translated into multiple languages or so that hearing impaired people can read your content. Additionally, let’s say you want to improve the discovery of your podcasts through search engine optimization. Transcribing your podcast will enable search engines to index the text, making it easier to find it. \n",
    "\n",
    "The purpose of this Guided Project is to introduce you to ASR (automatic speech recognition) system, to understand how the signal processing works, the architecture of a transformer model behind ASR and some examples on how to easily recognize, transcribe and translate some of your audio and video files, using publicly available ASR tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11861185-3834-47f9-a413-690ba925b269",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "\n",
    "* Packages to install\n",
    "* Libraries to Import\n",
    "* Background\n",
    "* Loading Models\n",
    "* Loading File\n",
    "* MEL Scale and MEL Spectogram\n",
    "* Language Detection\n",
    "* Decoding and Transcribing\n",
    "* Translation\n",
    "* Transcription From Youtube\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062d613b-a828-4532-9951-373b2122227a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Must be installed packages :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a8f849-366b-4351-925f-3cba67297abf",
   "metadata": {},
   "source": [
    "#### Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fb0e6b-1d03-477a-a714-ccddd5420b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2744f7f-f263-43e8-8270-39a044a26b40",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34aa838-bbd7-47c0-b750-86b8a6c5f382",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/librosa/librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e118158-d4bb-401e-9cdc-38478bb141ce",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeef1f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import whisper\n",
    "import pytube\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549cada6-5ecb-4648-b306-064144fca098",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Background (optional)\n",
    "Whisper application is a new non-commercial ASR (automatic speech recognition) system that was recently made available on Open AI. The Whisper model was proposed in Robust Speech Recognition via Large-Scale Weak Supervision article by Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.Whisper has been trained for 680,000 hours on huge amount of speech data collected from the internet. The diverse dataset allows Whisper to understand different accents, and filter background noise.Whisper is a multi-task model which is based on a encoder-decoder transformer architecture. While training the model, the input data (i.e. the audio file) is split into 30 seconds parts and converted into log-mel spectrogram, which is fed to the encoder and the decoder and is responsible for predicting the corresponding text and translating it into multiple languages. About one-third of the audio data is non-english. Keeping the dataset diverse has helped the team gain better performance than other supervised state-of-the-art models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb61f32-4cc2-45b1-9d91-17a4831e9706",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading the models\n",
    "There are five model sizes, four with English-only versions, offering speed and accuracy trade-offs.\n",
    " You can use the tiny model for light weight applications, the large model if accuracy is most important, and the base or medium models for everything in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f33540c-dc6b-4cb1-bd61-31d4fdda5d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the tiny size model:\n",
    "#model_t = whisper.load_model(\"tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b8182c-8a78-4c28-a24a-3c3744c5dd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the base size model\n",
    "#model = whisper.load_model(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b608dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the medium size model\n",
    "model_m = whisper.load_model(\"medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bc5372-199f-48e7-856f-de7e24ff51de",
   "metadata": {},
   "source": [
    "### Loading the file\n",
    "We start by loading an .mp4 audio file, previously uploaded to the IBM Cloud Object Storage.\n",
    "To do so, we define the file path: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bc0bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0EPMEN/20220627_140242.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d68d6cc-b9c0-4e45-a868-c7c2276bd05f",
   "metadata": {},
   "source": [
    "Load an audio file, using `load_audio()` function. If you are using your own file, you can replace the 'file_path' with an actual name of your file, e.g., 'podcast.mp3'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328467dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio_35 = whisper.load_audio(file_path)\n",
    "audio_35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7949f997-22ee-40f1-ba09-658c1198d791",
   "metadata": {},
   "source": [
    "The output above is basically the amplitude of sound waves, or the relative strength of sound waves. It is in the form of a numpy array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14faee10-cfa5-4920-908a-0bb4415b1f02",
   "metadata": {},
   "source": [
    "Now, we can find the sampling interval, the distance or time between the measurements. The total time of audio sample is 35 seconds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2b5422",
   "metadata": {},
   "outputs": [],
   "source": [
    "T=35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c6c4ca-5629-4d26-96b8-3f4a87c9bcda",
   "metadata": {},
   "source": [
    "We check how many samples are in our audio file by calling the `shape()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203c2ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples=audio_35.shape[0]\n",
    "n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e064a73-6b24-45fc-aac4-ad973cbf714a",
   "metadata": {},
   "source": [
    "There are 559445 of samples in 35 seconds audio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbac7f3-c79d-443d-9085-ea4341a8eeaa",
   "metadata": {},
   "source": [
    "Now, we can find the time between samples by dividing the total time by the number of samples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c4538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta=T/n_samples\n",
    "delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3201af-5141-4289-97ad-9b60cba9eb68",
   "metadata": {},
   "source": [
    "The time between samples is 6.25620034140979e-05. Now, we can get the sampling frequency: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc1f2d5-b75e-48a0-9e14-ad4d1fb71e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs=1/delta\n",
    "Fs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba88c748-2fb4-4860-9e4c-1817d67709d9",
   "metadata": {},
   "source": [
    "Now, we can get the time of each sample: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37ff957-c25f-4e36-8b5c-4e5c21db2cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "time=np.linspace(0,(n_samples-1)*delta,n_samples)\n",
    "time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92893d37-f2a6-46ce-abae-e25251398203",
   "metadata": {},
   "source": [
    "Finally, we can plot the amplitude with respect to time: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0accde-60d3-4d60-a06a-403335a48f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Signal')\n",
    "plt.plot(time,audio_35 )\n",
    "plt.ylabel('amplitude')\n",
    "plt.xlabel('seconds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f16ca4b-b1fd-48f0-aae3-d26ba74a1c71",
   "metadata": {},
   "source": [
    "Above is a waveform for the signal. Next, we can use the `pad_or_trim()` method to ensure the sample is in the right form for inference. In our case the file is 35 seconds, so it gets trimmed to fit the 30 seconds part (30 seconds parts get fed into the encoder).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0a1425-6c65-4fb0-8e79-970710fc2429",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = whisper.pad_or_trim(audio_35)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043741e0-f5b1-4c26-b8b8-e0c0bb91b1d3",
   "metadata": {},
   "source": [
    "We can plot the amplitude of signal over time with trimmed/padded audio:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e242830-f131-4b6b-8cc2-a9f6b4ea8cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples=audio.shape[0]\n",
    "time=np.linspace(0,(n_samples-1)*delta,n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac24884-9c9d-4c10-911e-f87e4d8010da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time,audio)\n",
    "\n",
    "plt.ylabel('amplitude')\n",
    "plt.xlabel('seconds')\n",
    "plt.title('Signal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa632ee-daee-4880-ad8c-4f1e07915507",
   "metadata": {},
   "source": [
    "### Mel scale and mel spectrogram\n",
    "\n",
    "Studies have shown that humans do not perceive frequencies on a linear scale. We are better at detecting differences in lower frequencies than higher frequencies. For example, we can easily tell the difference between 500 and 1000 Hz, but we will hardly be able to tell a difference between 10,000 and 10,500 Hz, even though the distance between the two pairs are the same.\n",
    "In 1937, Stevens, Volkmann, and Newmann proposed a unit of pitch such that equal distances in pitch sounded equally distant to the listener. So, we need to convert frequencies to **mel scale**, so that sounds of equal distance from each other also “sound” to humans as they are equal in distance from one another.\n",
    "\n",
    "A **mel spectrogram** is a spectrogram where the frequencies are converted to the mel scale.\n",
    "\n",
    "*librosa* library has a wrapper for mel spectrograms in its API that can be used directly. However, here, we will use a simpler mathematical method to produce a mel spectrogram. \n",
    "\n",
    "\n",
    "\n",
    "[Understanding the Mel Spectrogram](https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0EPMEN1227-2022-01-01) and [How to Create & Understand Mel-Spectrograms](https://importchris.medium.com/how-to-create-understand-mel-spectrograms-ff7634991056?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0EPMEN1227-2022-01-01) articles have more information about sound interpretation and mel spectrograms.\n",
    "\n",
    "We can now start plotting a mel spectrogram by applying a `log_mel_spectrogram()` function to our audio file. It converts the y-axis (frequency) into the mel scale:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facfeea0-e2a7-48ba-a2c4-58bb13e1bb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel = whisper.log_mel_spectrogram(audio).to(model_m.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f44d8bb-315f-41ef-8431-530920b19663",
   "metadata": {},
   "source": [
    "The output above is a tensor of converted frequencies. Now, we plot 2 subplots, one is a regular representation of sound amplitude over period of time, and the other is our mel spectrogram:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a23b6e-c0b9-42f3-8662-b89eb2be31ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2)\n",
    "fig.tight_layout(pad=5.0)\n",
    "ax1.plot(time,audio)\n",
    "ax1.set_title('Signal')\n",
    "ax1.set_xlabel('Time, seconds')\n",
    "ax1.set_ylabel('Amplitude')\n",
    "ax2.imshow((mel.numpy()*mel.numpy())**(1/2),interpolation='nearest', aspect='auto')\n",
    "ax2.set_title('Mel Spectrogram of a Signal')\n",
    "ax2.set_xlabel('Time, seconds')\n",
    "ax2.set_ylabel('Mel Scale')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f24b6b-14cb-4dcf-a679-e92197ff4883",
   "metadata": {},
   "source": [
    "### Language Detection\n",
    "In this Example, we will listen to our audio file and detect the spoken language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eb40fa-f534-4092-91a7-9348f45add7d",
   "metadata": {},
   "source": [
    "The sample rate (sr) by default is 22050, which means that for every second there are 22,050 samples. We can use `ipd.Audio()` function to listen to our audio file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cce759-c149-4369-be0e-0da53289212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr=22050\n",
    "ipd.Audio(audio, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ac82a6-fc06-4d1b-8b8e-f21cc4204c84",
   "metadata": {},
   "source": [
    "We can find the probability of each language by using `detect_language()` method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc447a58-d447-4dcf-995f-79e14900c5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, probs = model_m.detect_language(mel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5af35a-5624-4e92-b814-4774eabc5972",
   "metadata": {},
   "source": [
    "We also can print the top ten languages' prefixes and their probabilities:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302e9c1f-81fb-4735-89dc-11cb5bded5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([item  for item in  probs.items()][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b8b969-9f88-4d7e-9c68-b0af3c68e5f8",
   "metadata": {},
   "source": [
    "Finally, we can detect the spoken language by selecting the key with the highest probability value:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94316000-286d-4601-ac44-af09c9712463",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Detected language: {max(probs, key=probs.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fce04df-5d67-43cc-b8e0-cb75025efa36",
   "metadata": {},
   "source": [
    "Therefore, the spoken language is English, with 99.97% probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c0962e-9174-4618-b14e-b7d01f322f88",
   "metadata": {},
   "source": [
    "### Decoding and Transcription\n",
    "\n",
    "The difference between decoding and transcription is that the decode function processes only 30 seconds of audio segment. Transcribe function will decode the entire audio file. Below, we decode 30-seconds audio segment(s) using `whisper.decode()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405db303-166a-4594-adbe-6978edf8faa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = whisper.DecodingOptions(fp16 = False)\n",
    "result = whisper.decode(model_m, mel, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30a3f22-4bc8-4a97-a62c-10f44a228c9e",
   "metadata": {},
   "source": [
    "We print the recognized text using the attribute text :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9026c3b-6f97-46f3-9699-f19151f6ac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b58bf4-1b49-462e-8218-2b60d33ccf5f",
   "metadata": {},
   "source": [
    "The output of the above is a text that fits into 30 seconds audio segment. Now, the `transcribe()` method reads the entire file and processes the audio with a sliding 30-second window, performing autoregressive sequence-to-sequence predictions on each window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e604745a-bd6a-4d36-8bf3-696cb2595128",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = model_m.transcribe(file_path, fp16 = False)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35433f7c-0a86-4611-b1d0-122b0db8088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec5c5de-aa1a-484d-84d1-bfa1785e533e",
   "metadata": {},
   "source": [
    "### Translation\n",
    "\n",
    "In this Example, we translate our audio file to French, by setting `language='fr'`. You can also use any other language available [here](https://github.com/openai/whisper).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fd9c76-aefc-435b-bd9c-2ef744aa8797",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = model_m.transcribe(file_path, language='fr', fp16 = False)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ad6bb6-9157-4f2d-b734-9ad93c04001c",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89501271-9d90-4b39-9670-72eec59f2160",
   "metadata": {},
   "source": [
    "### Transcription from YouTube\n",
    "\n",
    "Below, we will select a random YouTube video and read it using the `pytube()` library. This one is a 30 seconds Motivational Speech.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52820555-e79c-43f6-9ed1-987bb2b23738",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_url = \"https://www.youtube.com/watch?v=E9lAeMz1DaM\"\n",
    "data = pytube.YouTube(video_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd235551-6ff1-4fc4-969c-0c31a97d0b2e",
   "metadata": {},
   "source": [
    "We will convert and download an 'MP4' file using `streams.get_audio_only()` and `download()` functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247ccdf9-728f-4297-8296-391bb019b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = data.streams.get_audio_only()\n",
    "audio_file=speech.download()\n",
    "print(\"audio file path:\",audio_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f4c053-5370-481c-b92e-232f06c57b8e",
   "metadata": {},
   "source": [
    "Finally, we will transcribe and translate the output to Japanese language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1648bf67-93ba-426d-9368-2729ff932f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_m.transcribe(audio_file,fp16 = False,language='ja')[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75ec9f5-87b5-409e-a3a2-b9bebeffcd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7c8ddd-4692-4fde-a14c-8284c4118080",
   "metadata": {},
   "source": [
    "**Author**\n",
    "\n",
    "Alireza Hosseinzadeh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
